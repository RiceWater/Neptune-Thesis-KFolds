{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "348ca800-674e-4b34-9d14-551b8c226b08",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79947b1a-22c4-4658-9375-a7bcbc52590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import os\n",
    "import cv2\n",
    "import imghdr\n",
    "import image_to_numpy\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "from numpy.random import default_rng\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomContrast, RandomBrightness, Rescaling, Resizing\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, ReLU\n",
    "from tensorflow.keras.metrics import F1Score, Precision, Recall, CategoricalAccuracy\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from time import perf_counter\n",
    "import math\n",
    "\n",
    "import neptune\n",
    "from neptune.integrations.tensorflow_keras import NeptuneCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb1457f-83b0-4eb9-8a55-57299e4e4aac",
   "metadata": {},
   "source": [
    "# Initialize Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c9f594-9465-4546-8a43-7f7ddefbce80",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"V4.0.1\"\n",
    "checkpoint_path = f'./checkpoints/{model_name}/FOLD-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cee80d-5b28-441a-8ec0-67da468c4966",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(params):\n",
    "    model = Sequential()\n",
    "    model.add(RandomFlip(mode='horizontal', seed=1, input_shape=(256,256,3)))\n",
    "    model.add(RandomRotation(factor=0.2, seed=1))\n",
    "\n",
    "    model.add(Conv2D(input_shape=(256,256,3), filters=16, kernel_size=(3,3), padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(ReLU())\n",
    "    model.add(MaxPooling2D())\n",
    "\n",
    "    model.add(Conv2D(filters=16, kernel_size=(3,3), padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(ReLU())\n",
    "    model.add(MaxPooling2D())\n",
    "\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3,3), padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(ReLU())\n",
    "    model.add(MaxPooling2D())\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.2, seed=1))\n",
    "    model.add(Dense(units=128,activation=\"relu\"))\n",
    "    model.add(Dense(units=64,activation=\"relu\"))\n",
    "\n",
    "    model.add(Dense(units=4, activation=\"softmax\"))\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=params[\"lr\"],\n",
    "    )\n",
    "    \n",
    "    model.compile('adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022580a2-bbb0-46de-80d2-b93e647275c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_callbacks(counter):\n",
    "    neptune_cbk = NeptuneCallback(run=run, base_namespace=\"training\")\n",
    "\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path + str(counter), \n",
    "                                                                 monitor='val_loss', \n",
    "                                                                 mode='auto', \n",
    "                                                                 save_best_only=True, \n",
    "                                                                 verbose=1)\n",
    "    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                                   patience=10,\n",
    "                                                  )\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n",
    "                                                         factor=0.5, \n",
    "                                                         patience=5, \n",
    "                                                         min_delta=0.001, \n",
    "                                                         mode='auto', \n",
    "                                                         verbose=1)\n",
    "    return [neptune_cbk, reduce_lr, cp_callback, es_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d16a35-875d-4cff-abd0-6f554224f314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(hist):\n",
    "    fig = plt.figure(figsize=(3,2))\n",
    "    plt.plot(hist.history['loss'], color='teal', label='loss')\n",
    "    plt.plot(hist.history['val_loss'], color='orange', label='val_loss')\n",
    "    plt.suptitle('Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba83bd7-0b4b-43fc-8484-9f695a1f0039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc(hist):\n",
    "    fig = plt.figure(figsize=(3,2))\n",
    "    plt.plot(hist.history['accuracy'], color='teal', label='acc')\n",
    "    plt.plot(hist.history['val_accuracy'], color='orange', label='val_acc')\n",
    "    plt.suptitle('Accuracy')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b527cae-cd2f-4cf0-8ad8-03355050b95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_rate(metric_list):\n",
    "    return sum(metric_list) / len(metric_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27ee883-af64-4f8a-980d-d7936a9d6c9b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bf8dc5-2d83-4b0a-ab84-007f05ccaa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Building', 'Plant', 'Road', 'Vehicle']\n",
    "X_train = []\n",
    "y_train = []\n",
    "y_label_idx = []\n",
    "image_size = 256\n",
    "for i in labels:\n",
    "    folderPath = os.path.join('D:\\DATASETS\\For-Thesis\\Labels', i)\n",
    "    for j in os.listdir(folderPath):\n",
    "        img = image_to_numpy.load_image_file(os.path.join(folderPath,j))\n",
    "        img = cv2.resize(img,(image_size, image_size))\n",
    "        X_train.append(img)\n",
    "        y_label_idx.append(labels.index(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dd841d-f1af-4c0a-9557-55d800db547b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_label_idx = np.array(y_label_idx)\n",
    "\n",
    "X_train_scaled = X_train.astype(np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569494d1-0259-4eaa-bcbe-89a567ac7203",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = default_rng(seed=1)\n",
    "\n",
    "buildings_test_idx = rng.choice(np.arange(0,600), size=120, replace=False)\n",
    "plants_test_idx = rng.choice(np.arange(600,1200), size=120, replace=False)\n",
    "roads_test_idx = rng.choice(np.arange(1200,1800), size=120, replace=False)\n",
    "vehicles_test_idx = rng.choice(np.arange(1800,2400), size=120, replace=False)\n",
    "total_test_idx = np.concatenate((buildings_test_idx, plants_test_idx, roads_test_idx, vehicles_test_idx), axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b38d76e-94d9-442c-8025-11352e7a091d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_X(X_train_scaled):    \n",
    "    test_x = X_train_scaled[total_test_idx]\n",
    "    train_x = np.delete(X_train_scaled, total_test_idx, axis=0)\n",
    "    return train_x, test_x\n",
    "\n",
    "def train_test_y(y_label_idx):\n",
    "    test_y = y_label_idx[total_test_idx]\n",
    "    train_y = np.delete(y_label_idx, total_test_idx, axis=0)\n",
    "    return train_y, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0d422e-be3d-4544-84f8-1b5f39fd52af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, holdout_x = train_test_X(X_train_scaled)\n",
    "train_y, holdout_y = train_test_y(y_label_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746890dd-29f8-433f-8a17-1b8e9444ce77",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_x), len(holdout_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b2ae4a-5a0e-439d-9ece-c5a8a3d2fecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c75f07f-09ee-4037-83fb-d2bea64c4b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INDICES USED====================================\n",
    "# with open('total_test_idx.txt', 'w+') as f:\n",
    "     \n",
    "#     # write elements of list\n",
    "#     for items in total_test_idx:\n",
    "#         f.write('%s ' %items)\n",
    "     \n",
    "#     print(\"File written successfully\")\n",
    " \n",
    " \n",
    "# # close the file\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabf3dcc-8327-4f9b-af3a-aa795f1ee122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist = tf.keras.datasets.mnist\n",
    "# (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10b3d75-dfe3-4d66-a5c7-b46073861043",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Run KFOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c647dc-35a6-4391-80b6-1db5643a1820",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=8, shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4679eae-6da1-4f77-8dd2-bf6b2a248908",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"New-Thesis/KFold-Test\"\n",
    "api_token = \"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJiM2YzNGEwOC1iNmYyLTQ3YTEtYTljNi0xOWNmYWE0ZjZjOTMifQ==\"\n",
    "params = {\n",
    "    \"lr\": 0.001, \n",
    "    \"epochs\": 25, \n",
    "    \"batch_size\": 8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13eae77-d4a9-4c8a-8571-b1249e13a124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2061b0ab-2132-4f3b-94cc-d5584da402ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Log performance of each fold\n",
    "# ============================\n",
    "\n",
    "precisions = []\n",
    "recalls = []\n",
    "accuracies = []\n",
    "f1_blds = []\n",
    "f1_plants = []\n",
    "f1_roads = []\n",
    "f1_vhcls = []\n",
    "\n",
    "counter = 0\n",
    "start = perf_counter()\n",
    "for train, test in kfold.split(train_x, train_y):\n",
    "    start_fold = perf_counter()\n",
    "\n",
    "    run = neptune.init_run(\n",
    "        name= f\"{model_name}-FOLD_{counter}\",\n",
    "        project=project_name,\n",
    "        api_token=api_token,\n",
    "    )  # your credentials\n",
    "\n",
    "    run[\"parameters\"] = params\n",
    "    model = make_model(params)\n",
    "    cat_y = tf.keras.utils.to_categorical(train_y)\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_x[train], cat_y[train]))\n",
    "    train_fold = train_dataset.shuffle(buffer_size=train_dataset.cardinality(), seed=1).batch(params['batch_size'])\n",
    "    \n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((train_x[test], cat_y[test]))\n",
    "    test_fold = test_dataset.shuffle(buffer_size=test_dataset.cardinality(), seed=1).batch(params['batch_size'])\n",
    "    \n",
    "    hist = model.fit(train_fold, \n",
    "                     validation_data=test_fold,\n",
    "                     batch_size=params['batch_size'],\n",
    "                     epochs=params['epochs'],\n",
    "                     verbose=1,\n",
    "                     callbacks=create_callbacks(counter))\n",
    "    end_fold = perf_counter()\n",
    "\n",
    "    # PLOTTING ==================================================\n",
    "    run[\"fig-train_loss\"].upload(plot_loss(hist))\n",
    "    run[\"fig-train_acc\"].upload(plot_acc(hist))\n",
    "    \n",
    "    # EVALUATING ==================================================\n",
    "    model = tf.keras.models.load_model(checkpoint_path + str(counter))\n",
    "    \n",
    "    pre = Precision()\n",
    "    rec = Recall()\n",
    "    c_acc = CategoricalAccuracy()\n",
    "    f1_score = F1Score()\n",
    "    for batch in test_fold.as_numpy_iterator():\n",
    "        X, y = batch\n",
    "        yhat = model.predict(X, verbose=0)\n",
    "        f1_score.update_state(y, yhat)\n",
    "        pre.update_state(y, yhat)\n",
    "        rec.update_state(y, yhat)\n",
    "        c_acc.update_state(y, yhat)\n",
    "        \n",
    "    precisions.append(pre.result().numpy() * 100)\n",
    "    recalls.append(rec.result().numpy() * 100)\n",
    "    accuracies.append(c_acc.result().numpy() * 100)\n",
    "    f1_s = f1_score.result().numpy() * 100\n",
    "    \n",
    "    f1_blds.append(f1_s[0])\n",
    "    f1_plants.append(f1_s[1])\n",
    "    f1_roads.append(f1_s[2])\n",
    "    f1_vhcls.append(f1_s[3])\n",
    "    \n",
    "    run[\"eval/ACC/\"] = accuracies[counter]\n",
    "    run[\"eval/REC/\"] = recalls[counter]\n",
    "    run[\"eval/PRE/\"] = precisions[counter]\n",
    "    run[\"eval/F1-Bld\"] = f1_blds[counter]\n",
    "    run[\"eval/F1-Plant\"] = f1_plants[counter]\n",
    "    run[\"eval/F1-Road\"] = f1_roads[counter]\n",
    "    run[\"eval/F1-Vhcl\"] = f1_vhcls[counter]\n",
    "    \n",
    "    # PRINTING RESULTS ===========================================\n",
    "    print(f'Fold Training Time: {math.floor((end_fold-start_fold)/3600) } hour(s) and {math.floor((end_fold-start_fold)/60)%60} mins')\n",
    "    print(f'Precision: {precisions[counter]}  -  Recall: {recalls[counter]}')\n",
    "    print(f'Accuracy: {accuracies[counter]}')\n",
    "    print(f'F1Score: Building {f1_blds[counter]} | Plant {f1_plants[counter]} | Road {f1_roads[counter]} | Vehicle {f1_vhcls[counter]}')\n",
    "    \n",
    "    run.stop()\n",
    "    \n",
    "    counter += 1\n",
    "\n",
    "end = perf_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdcaea3-c498-44dd-be65-c899e9ba695c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Get Average Performance of Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6a42b9-a558-4ea3-a099-c111c57b7ccf",
   "metadata": {},
   "source": [
    "## Save to Neptune AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13e1214-9537-4e9a-9e85-9b2f481506c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = neptune.init_run(\n",
    "        name= f\"{model_name}-AVG\",\n",
    "        project=project_name,\n",
    "        api_token=api_token,\n",
    "    )  # your credentials\n",
    "\n",
    "run[\"eval/PRE/\"] = avg_rate(precisions)\n",
    "run[\"eval/REC/\"] = avg_rate(recalls)\n",
    "run[\"eval/ACC/\"] = avg_rate(accuracies)\n",
    "run[\"eval/F1-Bld\"] = avg_rate(f1_blds)\n",
    "run[\"eval/F1-Plant\"] = avg_rate(f1_plants)\n",
    "run[\"eval/F1-Road\"] = avg_rate(f1_roads)\n",
    "run[\"eval/F1-Vhcl\"] = avg_rate(f1_vhcls)\n",
    "\n",
    "run.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69c613b-b1b9-4a1b-9ecf-969e62123fc9",
   "metadata": {},
   "source": [
    "## Save to Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ea4d93-bce3-4a78-a309-280b83d61676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Command to access TensorBoard HParams:\n",
    "\n",
    "# tensorboard --logdir PATH/logs/hparam_tuning\n",
    "with tf.summary.create_file_writer(f'./logs/hparam_tuning/{model_name}').as_default():\n",
    "    hparams = {\n",
    "        'CONV_F1': 16,\n",
    "        'CONV_F2': 16,\n",
    "        'CONV_F3': 32,\n",
    "        'DROPOUT': 0.2,\n",
    "        'D_UNITS_1': 128,\n",
    "        'D_UNITS_2': 96,\n",
    "    }\n",
    "    hp.hparams(hparams)\n",
    "    tf.summary.scalar('AVG. PRE', avg_rate(precisions), step=1)\n",
    "    tf.summary.scalar('AVG. REC', avg_rate(recalls), step=1)\n",
    "    tf.summary.scalar('AVG. ACC', avg_rate(accuracies), step=1)\n",
    "    tf.summary.scalar('AVG. F1 - Building', avg_rate(f1_blds), step=1)\n",
    "    tf.summary.scalar('AVG. F1 - Plant', avg_rate(f1_plants), step=1)\n",
    "    tf.summary.scalar('AVG. F1 - Road', avg_rate(f1_roads), step=1)\n",
    "    tf.summary.scalar('AVG. F1 - Vehicle', avg_rate(f1_vhcls), step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53362329-adc7-4db5-a223-a27367cbb06a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a48f537-6a6e-4696-afc1-c608ffc793f3",
   "metadata": {},
   "source": [
    "# Asd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e06983-0acb-4201-bb07-4c6579d25cbd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6590ec4c-5fc3-470e-bc7c-26503425ab8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model= make_model()\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_data=(x_test,y_test),\n",
    "    epochs=params[\"epochs\"],\n",
    "    batch_size=params[\"batch_size\"],\n",
    "    callbacks=create_callbacks(0),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64460324-7348-47a8-bfed-b23556965940",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "true_test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "true_test_set = true_test_dataset.shuffle(buffer_size=true_test_dataset.cardinality(), seed=1).batch(64)\n",
    "\n",
    "f1 = F1Score()\n",
    "pre = Precision()\n",
    "rec = Recall()\n",
    "c_acc = CategoricalAccuracy()\n",
    "for batch in true_test_set.as_numpy_iterator():\n",
    "    X, y = batch\n",
    "    yhat = model.predict(X)\n",
    "    yhat_max = []\n",
    "    for i in yhat:\n",
    "        maxed = np.argmax(i)\n",
    "        yhat_max.append(maxed)\n",
    "    yhat = np.array(yhat_max)\n",
    "    f1.update_state(y, yhat)\n",
    "    pre.update_state(y, yhat)\n",
    "    rec.update_state(y, yhat)\n",
    "    c_acc.update_state(y, yhat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4864d2-4fb0-4526-95d3-9421c7bfa082",
   "metadata": {},
   "outputs": [],
   "source": [
    "run[\"eval/ACC/\"] = float(c_acc.result().numpy() * 100)\n",
    "run[\"eval/REC/\"] = float(rec.result().numpy() * 100)\n",
    "run[\"eval/PRE/\"] = float(pre.result().numpy() * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9f5d4a-c095-4788-8b9c-0f67f2631f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153bb951-edfb-42a4-93ae-30f5d37fcbab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1becccd6-290d-486c-9f34-f5b4b025b4f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6ba075-09fd-4fc7-8886-c600b17d2938",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics = model.evaluate(x_test, y_test, verbose=0)\n",
    "for j, metric in enumerate(eval_metrics):\n",
    "    run[\"eval/AVG-ACC/{}\".format(model.metrics_names[j])] = metric\n",
    "\n",
    "run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09100e0c-0d94-4e3d-8202-ec6e2872234d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j, metric in enumerate(eval_metrics):\n",
    "    print(model.metrics_names[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f20526-bb52-4153-a291-c18829f55825",
   "metadata": {},
   "source": [
    "## Neptune AI Notes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "59ff9add-a7ad-4ccf-b4f4-e3b9dd83564d",
   "metadata": {},
   "source": [
    "1. neptune callback keeps track of learning rate - can use ReduceLROnPlateau\n",
    "2. neptune seems better used with one run each, but we can just create 1 more RUN before tuning the hyperparameters, name it with \"AVG\", have an \"AVG\" tag, and then save the average result\n",
    "3. The code below creates column in Neptune AI's website\n",
    "\n",
    "  for j, metric in enumerate(eval_metrics):\n",
    "    run[\"eval/AVG-ACC/{}\".format(model.metrics_names[j])] = metric\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2603ac1c-fe1d-4a21-b8f0-9d8a32e67462",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neptune",
   "language": "python",
   "name": "neptune"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
